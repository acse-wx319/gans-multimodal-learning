{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.dependencies import *\n",
    "from lib.time_series_dependencies import *\n",
    "from lib.models import *\n",
    "import os\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modes\n",
    "gp = True\n",
    "sn = False\n",
    "\n",
    "# steps\n",
    "train = True\n",
    "eph = '29999' # if not in training mode, specify a epoch of model to load\n",
    "fixed_input = True\n",
    "\n",
    "# specifications\n",
    "MODE = 'wgan-sn'  # wgan or wgan-gp\n",
    "DATASET = 'pollution'  # 8gaussians, sine, heteroscedastic, moon\n",
    "suffix = '_gp_lat64_20'\n",
    "DIM = 512  # 512 Model dimensionality\n",
    "LATENT_DIM = 64 # latent space dimension\n",
    "ntimes = 20 # window length for the GAN\n",
    "nvars = 7 # number of variables in time series data\n",
    "INPUT_DIM = ntimes*nvars # input dimension\n",
    "LAMBDA = 0.1  # Smaller lambda seems to help for toy tasks specifically\n",
    "DROPOUT_RATE = 0.1 # dropout rate for GAN\n",
    "lr = 1e-4 # learning rate for training GAN\n",
    "CRITIC_ITERS = 5  # How many critic iterations per generator iteration\n",
    "BATCH_SIZE = 140  # 256 Batch size\n",
    "ITERS = 5000  # how many generator iterations to train for\n",
    "log_interval = 1000 # how frequent to write to log and save models \n",
    "use_cuda = False\n",
    "\n",
    "DATA_PATH = '/Users/wendyhsu/Documents/ACSE/ACSE_9/data/'\n",
    "DATA_NAME = 'pollution_one_day.csv'\n",
    "# DATA_NAME = 'pollution_one_day2.csv'\n",
    " \n",
    "TMP_PATH = 'tmp/' + DATASET + suffix + '/'\n",
    "if not os.path.isdir(TMP_PATH):\n",
    "    os.makedirs(TMP_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data file: /Users/wendyhsu/Documents/ACSE/ACSE_9/data/pollution_one_day.csv\n"
     ]
    }
   ],
   "source": [
    "netG = Generator(LATENT_DIM, DIM, DROPOUT_RATE, INPUT_DIM)\n",
    "if sn:\n",
    "    netD = DiscriminatorSNTime(DIM, INPUT_DIM, BATCH_SIZE)\n",
    "else:\n",
    "    netD = DiscriminatorTime(DIM, INPUT_DIM, BATCH_SIZE)\n",
    "\n",
    "netG.apply(weights_init)\n",
    "netD.apply(weights_init)\n",
    "\n",
    "# read data\n",
    "df, scaler = read_data(DATA_PATH+DATA_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wendyhsu/Documents/ACSE/ACSE_9/gans-multimodal-learning/lib/time_series_dependencies.py:80: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, ax = plt.subplots(1, 1, figsize=(6, 4))\n",
      "/Users/wendyhsu/Documents/ACSE/ACSE_9/gans-multimodal-learning/lib/time_series_dependencies.py:88: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, ax = plt.subplots(1, 1, figsize=(6, 4))\n",
      "<ipython-input-6-5efe396a3c82>:135: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, ax = plt.subplots(1, 1, figsize=[10, 5])\n",
      "<ipython-input-6-5efe396a3c82>:142: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, ax = plt.subplots(1, 1, figsize=[10, 5])\n",
      "/Users/wendyhsu/Documents/ACSE/ACSE_9/gans-multimodal-learning/lib/time_series_dependencies.py:48: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, ax = plt.subplots(1, 1, figsize=(6, 4))\n",
      "/Users/wendyhsu/Documents/ACSE/ACSE_9/gans-multimodal-learning/lib/time_series_dependencies.py:56: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, ax = plt.subplots(1, 1, figsize=(6, 4))\n",
      "/Users/wendyhsu/Documents/ACSE/ACSE_9/gans-multimodal-learning/lib/time_series_dependencies.py:64: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, ax = plt.subplots(1, 1, figsize=(6, 4))\n",
      "/Users/wendyhsu/Documents/ACSE/ACSE_9/gans-multimodal-learning/lib/time_series_dependencies.py:72: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, ax = plt.subplots(1, 1, figsize=(6, 4))\n",
      "/Users/wendyhsu/Documents/ACSE/ACSE_9/gans-multimodal-learning/lib/time_series_dependencies.py:80: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, ax = plt.subplots(1, 1, figsize=(6, 4))\n",
      "/Users/wendyhsu/Documents/ACSE/ACSE_9/gans-multimodal-learning/lib/time_series_dependencies.py:88: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, ax = plt.subplots(1, 1, figsize=(6, 4))\n",
      "<ipython-input-6-5efe396a3c82>:135: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, ax = plt.subplots(1, 1, figsize=[10, 5])\n",
      "<ipython-input-6-5efe396a3c82>:142: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, ax = plt.subplots(1, 1, figsize=[10, 5])\n"
     ]
    }
   ],
   "source": [
    "if train:\n",
    "    # start writing log\n",
    "    f = open(TMP_PATH + \"log.txt\", \"w\")\n",
    "    # print specifications\n",
    "    f.write('gradient penalty: ' + str(gp))\n",
    "    f.write('\\n spectral normalization: ' + str(sn))\n",
    "    f.write('\\n datasest: ' + DATASET)\n",
    "    f.write('\\n hidden layer dimension: ' + str(DIM))\n",
    "    f.write('\\n latent space dimension: ' + str(LATENT_DIM))\n",
    "    f.write('\\n gradient penalty lambda: ' + str(LAMBDA))\n",
    "    f.write('\\n dropout rate: ' + str(DROPOUT_RATE))\n",
    "    f.write('\\n critic iterations per generator iteration: ' + str(CRITIC_ITERS))\n",
    "    f.write('\\n batch size: ' + str(BATCH_SIZE))\n",
    "    f.write('\\n total iterations: ' + str(ITERS))\n",
    "    f.write('\\n')\n",
    "    # print model structures\n",
    "    f.write(str(netG))\n",
    "    f.write(str(netD))\n",
    "    f.write('\\n')\n",
    "\n",
    "    if use_cuda:\n",
    "        netD = netD.cuda()\n",
    "        netG = netG.cuda()\n",
    "    \n",
    "    optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(0.5, 0.9))\n",
    "    optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(0.5, 0.9))\n",
    "    \n",
    "    one = torch.FloatTensor([1])\n",
    "    mone = one * -1\n",
    "    if use_cuda:\n",
    "        one = one.cuda()\n",
    "        mone = mone.cuda()\n",
    "    \n",
    "    # create time window data\n",
    "    X_train = torch.Tensor(concat_timesteps(df, ntimes, 1, df.shape[0]))\n",
    "    \n",
    "    losses = []\n",
    "    wass_dist = []\n",
    "    \n",
    "    # start timing\n",
    "    start = timeit.default_timer()\n",
    "    batch = 0\n",
    "    \n",
    "    for iteration in range(ITERS):\n",
    "        while (batch+1)*BATCH_SIZE <= X_train.shape[0]:\n",
    "            ############################\n",
    "            # (1) Update D network\n",
    "            ###########################\n",
    "            for iter_d in range(CRITIC_ITERS):\n",
    "                if (batch+1)*BATCH_SIZE  <= X_train.shape[0]:\n",
    "                    # _data = next(data).float()\n",
    "                    _data = X_train[batch*BATCH_SIZE:(batch+1)*BATCH_SIZE, :, :]\n",
    "                    batch += 1\n",
    "                    if use_cuda:\n",
    "                        _data = _data.cuda()\n",
    "            \n",
    "                    netD.zero_grad()\n",
    "            \n",
    "                    # train with real\n",
    "                    D_real = netD(_data)\n",
    "                    D_real = D_real.mean().unsqueeze(0)\n",
    "                    D_real.backward(mone)\n",
    "            \n",
    "                    # train with fake\n",
    "                    noise = torch.randn(BATCH_SIZE, LATENT_DIM)\n",
    "                    if use_cuda:\n",
    "                        noise = noise.cuda()\n",
    "                    fake = netG(noise)\n",
    "                    D_fake = netD(fake.detach())\n",
    "                    D_fake = D_fake.mean().unsqueeze(0)\n",
    "                    D_fake.backward(one)\n",
    "            \n",
    "                    # train with gradient penalty\n",
    "                    if gp:\n",
    "                        gradient_penalty = calc_gradient_penalty(netD, _data.view(-1, BATCH_SIZE, INPUT_DIM), fake, BATCH_SIZE, LAMBDA, use_cuda)\n",
    "                        gradient_penalty.backward()\n",
    "                    \n",
    "                    if gp:\n",
    "                        D_cost = abs(D_fake - D_real) + gradient_penalty\n",
    "                    else:\n",
    "                        D_cost = abs(D_fake - D_real)\n",
    "                        \n",
    "                    Wasserstein_D = abs(D_real - D_fake)\n",
    "                    optimizerD.step()\n",
    "        \n",
    "            ############################\n",
    "            # (2) Update G network\n",
    "            ############################\n",
    "            netG.zero_grad()\n",
    "            if (batch+1)*BATCH_SIZE  <= X_train.shape[0]:\n",
    "                    # _data = next(data).float()\n",
    "                    _data = X_train[batch*BATCH_SIZE:(batch+1)*BATCH_SIZE, :, :]\n",
    "            # _data = torch.Tensor(X_train[batch*BATCH_SIZE:(batch+1)*BATCH_SIZE, :, :])\n",
    "            # batch += 1\n",
    "            # _data = next(data).float()\n",
    "            # if use_cuda:\n",
    "            #     _data = _data.cuda()\n",
    "        \n",
    "            noise = torch.randn(BATCH_SIZE, LATENT_DIM)\n",
    "            if use_cuda:\n",
    "                noise = noise.cuda()\n",
    "            fake = netG(noise)\n",
    "            G = netD(fake)\n",
    "            G = G.mean().unsqueeze(0)\n",
    "            G.backward(mone)\n",
    "            G_cost = -G\n",
    "            optimizerG.step()\n",
    "            \n",
    "            losses.append([G_cost.cpu().item(), D_cost.cpu().item()])\n",
    "            wass_dist.append(Wasserstein_D.cpu().item())\n",
    "            \n",
    "        # reset batch \n",
    "        batch = 0\n",
    "\n",
    "        if iteration % log_interval == log_interval - 1:\n",
    "            # save discriminator model\n",
    "            torch.save(netD.state_dict(), TMP_PATH + 'disc_model' + str(iteration) + '.pth')\n",
    "            # save generator model\n",
    "            torch.save(netG.state_dict(), TMP_PATH + 'gen_model' + str(iteration) + '.pth')\n",
    "            # report iteration number\n",
    "            f.write('Iteration ' + str(iteration) + '\\n')\n",
    "            # report time\n",
    "            stop = timeit.default_timer()\n",
    "            f.write('    Time spent: ' + str(stop - start) + '\\n')\n",
    "            # report loss\n",
    "            f.write('    Generator loss: ' + str(G_cost.cpu().item()) + '\\n')\n",
    "            f.write('    Discriminator loss: ' + str(D_cost.cpu().item()) + '\\n')\n",
    "            f.write('    Wasserstein distance: ' + str(Wasserstein_D.cpu().item()) + '\\n')\n",
    "            # save frame plot\n",
    "            noise = torch.randn(150, LATENT_DIM)\n",
    "            fake_data = netG(noise).detach().numpy().reshape(150, ntimes, nvars)\n",
    "            fake_data = scaler.inverse_transform(fake_data.reshape(150*ntimes, nvars))\n",
    "            plot_time_data(fake_data, str(iteration), TMP_PATH, scaler)\n",
    "            # save loss plot\n",
    "            fig, ax = plt.subplots(1, 1, figsize=[10, 5])\n",
    "            ax.plot(losses)\n",
    "            ax.legend(['Generator', 'Discriminator'])\n",
    "            plt.title('Generator Loss v.s Discriminator Loss')\n",
    "            ax.grid()\n",
    "            plt.savefig(TMP_PATH + 'loss_trend' + str(iteration) + '.png')\n",
    "            # save wassertein loss plot\n",
    "            fig, ax = plt.subplots(1, 1, figsize=[10, 5])\n",
    "            ax.plot(wass_dist)\n",
    "            plt.title('Wassertein Distance')\n",
    "            ax.grid()\n",
    "            plt.savefig(TMP_PATH + 'wass_dist' + str(iteration) + '.png')\n",
    "    \n",
    "    # close log file\n",
    "    f.close()\n",
    "    \n",
    "# if not in training mode, reload trained models\n",
    "else:\n",
    "   # load pretrained models  \n",
    "    netG.load_state_dict(torch.load(TMP_PATH + 'gen_model' + eph + '.pth'))\n",
    "    netD.load_state_dict(torch.load(TMP_PATH + 'disc_model' + eph + '.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (main): Sequential(\n",
       "    (0): Linear(in_features=140, out_features=512, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Linear(in_features=512, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if fixed_input:\n",
    "    pred_out = predict_fixed_continuous(df, 0, 1400)\n",
    "    np.save(TMP_PATH + 'pred_out.npy', pred_out)\n",
    "\n",
    "    end_index = pred_out.shape[0]\n",
    "    plot_predictions(df[:end_index, :], pred_out[:end_index, :], eph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([140, 20, 7])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
